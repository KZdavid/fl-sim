

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Federated learning algorithms &mdash; fl-sim 0.0.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="fl-sim 0.0.1 documentation" href="index.html"/>
        <link rel="next" title="Command line interface" href="cli.html"/>
        <link rel="prev" title="NullRegularizer" href="api/generated/fl_sim.regularizers.NullRegularizer.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> fl-sim
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Usage examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/nodes.html">fl_sim.nodes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/nodes.html#base-node-class">Base Node class</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.nodes.Node.html">Node</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.nodes.Node.html#fl_sim.nodes.Node"><code class="docutils literal notranslate"><span class="pre">Node</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/nodes.html#server-classes">Server classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.nodes.Server.html">Server</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.nodes.Server.html#fl_sim.nodes.Server"><code class="docutils literal notranslate"><span class="pre">Server</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.nodes.ServerConfig.html">ServerConfig</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.nodes.ServerConfig.html#fl_sim.nodes.ServerConfig"><code class="docutils literal notranslate"><span class="pre">ServerConfig</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/nodes.html#client-classes">Client classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.nodes.Client.html">Client</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.nodes.Client.html#fl_sim.nodes.Client"><code class="docutils literal notranslate"><span class="pre">Client</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.nodes.ClientConfig.html">ClientConfig</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.nodes.ClientConfig.html#fl_sim.nodes.ClientConfig"><code class="docutils literal notranslate"><span class="pre">ClientConfig</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/datasets.html">fl_sim.data_processing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#base-classes">Base classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedDataset.html">FedDataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedDataset.html#fl_sim.data_processing.FedDataset"><code class="docutils literal notranslate"><span class="pre">FedDataset</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedVisionDataset.html">FedVisionDataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedVisionDataset.html#fl_sim.data_processing.FedVisionDataset"><code class="docutils literal notranslate"><span class="pre">FedVisionDataset</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedNLPDataset.html">FedNLPDataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedNLPDataset.html#fl_sim.data_processing.FedNLPDataset"><code class="docutils literal notranslate"><span class="pre">FedNLPDataset</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#vision-datasets">Vision datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedCIFAR.html">FedCIFAR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedCIFAR.html#fl_sim.data_processing.FedCIFAR"><code class="docutils literal notranslate"><span class="pre">FedCIFAR</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedCIFAR100.html">FedCIFAR100</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedCIFAR100.html#fl_sim.data_processing.FedCIFAR100"><code class="docutils literal notranslate"><span class="pre">FedCIFAR100</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedEMNIST.html">FedEMNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedEMNIST.html#fl_sim.data_processing.FedEMNIST"><code class="docutils literal notranslate"><span class="pre">FedEMNIST</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedMNIST.html">FedMNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedMNIST.html#fl_sim.data_processing.FedMNIST"><code class="docutils literal notranslate"><span class="pre">FedMNIST</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedRotatedCIFAR10.html">FedRotatedCIFAR10</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedRotatedCIFAR10.html#fl_sim.data_processing.FedRotatedCIFAR10"><code class="docutils literal notranslate"><span class="pre">FedRotatedCIFAR10</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedRotatedMNIST.html">FedRotatedMNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedRotatedMNIST.html#fl_sim.data_processing.FedRotatedMNIST"><code class="docutils literal notranslate"><span class="pre">FedRotatedMNIST</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxFEMNIST.html">FedProxFEMNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxFEMNIST.html#fl_sim.data_processing.FedProxFEMNIST"><code class="docutils literal notranslate"><span class="pre">FedProxFEMNIST</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxMNIST.html">FedProxMNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxMNIST.html#fl_sim.data_processing.FedProxMNIST"><code class="docutils literal notranslate"><span class="pre">FedProxMNIST</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#nlp-datasets">NLP datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedShakespeare.html">FedShakespeare</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedShakespeare.html#fl_sim.data_processing.FedShakespeare"><code class="docutils literal notranslate"><span class="pre">FedShakespeare</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxSent140.html">FedProxSent140</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedProxSent140.html#fl_sim.data_processing.FedProxSent140"><code class="docutils literal notranslate"><span class="pre">FedProxSent140</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#synthetic-datasets">Synthetic datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedSynthetic.html">FedSynthetic</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedSynthetic.html#fl_sim.data_processing.FedSynthetic"><code class="docutils literal notranslate"><span class="pre">FedSynthetic</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#libsvm-datasets">LibSVM datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedLibSVMDataset.html">FedLibSVMDataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.FedLibSVMDataset.html#fl_sim.data_processing.FedLibSVMDataset"><code class="docutils literal notranslate"><span class="pre">FedLibSVMDataset</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/datasets.html#dataset-registry-utilities">Dataset registry utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.register_fed_dataset.html">fl_sim.data_processing.register_fed_dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.register_fed_dataset.html#fl_sim.data_processing.register_fed_dataset"><code class="docutils literal notranslate"><span class="pre">register_fed_dataset()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.list_fed_dataset.html">fl_sim.data_processing.list_fed_dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.list_fed_dataset.html#fl_sim.data_processing.list_fed_dataset"><code class="docutils literal notranslate"><span class="pre">list_fed_dataset()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.data_processing.get_fed_dataset.html">fl_sim.data_processing.get_fed_dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.data_processing.get_fed_dataset.html#fl_sim.data_processing.get_fed_dataset"><code class="docutils literal notranslate"><span class="pre">get_fed_dataset()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/models.html">fl_sim.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#convolutional-neural-networks-cnn">Convolutional neural networks (CNN)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNMnist.html">CNNMnist</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNMnist.html#fl_sim.models.CNNMnist"><code class="docutils literal notranslate"><span class="pre">CNNMnist</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNFEMnist.html">CNNFEMnist</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNFEMnist.html#fl_sim.models.CNNFEMnist"><code class="docutils literal notranslate"><span class="pre">CNNFEMnist</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNFEMnist_Tiny.html">CNNFEMnist_Tiny</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNFEMnist_Tiny.html#fl_sim.models.CNNFEMnist_Tiny"><code class="docutils literal notranslate"><span class="pre">CNNFEMnist_Tiny</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar.html">CNNCifar</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar.html#fl_sim.models.CNNCifar"><code class="docutils literal notranslate"><span class="pre">CNNCifar</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar_Small.html">CNNCifar_Small</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar_Small.html#fl_sim.models.CNNCifar_Small"><code class="docutils literal notranslate"><span class="pre">CNNCifar_Small</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar_Tiny.html">CNNCifar_Tiny</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.CNNCifar_Tiny.html#fl_sim.models.CNNCifar_Tiny"><code class="docutils literal notranslate"><span class="pre">CNNCifar_Tiny</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.ResNet18.html">ResNet18</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.ResNet18.html#fl_sim.models.ResNet18"><code class="docutils literal notranslate"><span class="pre">ResNet18</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.ResNet10.html">ResNet10</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.ResNet10.html#fl_sim.models.ResNet10"><code class="docutils literal notranslate"><span class="pre">ResNet10</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#recurrent-neural-networks-rnn">Recurrent neural networks (RNN)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.RNN_OriginalFedAvg.html">RNN_OriginalFedAvg</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.RNN_OriginalFedAvg.html#fl_sim.models.RNN_OriginalFedAvg"><code class="docutils literal notranslate"><span class="pre">RNN_OriginalFedAvg</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.RNN_StackOverFlow.html">RNN_StackOverFlow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.RNN_StackOverFlow.html#fl_sim.models.RNN_StackOverFlow"><code class="docutils literal notranslate"><span class="pre">RNN_StackOverFlow</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.RNN_Sent140.html">RNN_Sent140</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.RNN_Sent140.html#fl_sim.models.RNN_Sent140"><code class="docutils literal notranslate"><span class="pre">RNN_Sent140</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.RNN_Sent140_LITE.html">RNN_Sent140_LITE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.RNN_Sent140_LITE.html#fl_sim.models.RNN_Sent140_LITE"><code class="docutils literal notranslate"><span class="pre">RNN_Sent140_LITE</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#multilayer-perceptron-mlp">Multilayer perceptron (MLP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.MLP.html">MLP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.MLP.html#fl_sim.models.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.FedPDMLP.html">FedPDMLP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.FedPDMLP.html#fl_sim.models.FedPDMLP"><code class="docutils literal notranslate"><span class="pre">FedPDMLP</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/models.html#linear-models">Linear models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.LogisticRegression.html">LogisticRegression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.LogisticRegression.html#fl_sim.models.LogisticRegression"><code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.SVC.html">SVC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.SVC.html#fl_sim.models.SVC"><code class="docutils literal notranslate"><span class="pre">SVC</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.models.SVR.html">SVR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.models.SVR.html#fl_sim.models.SVR"><code class="docutils literal notranslate"><span class="pre">SVR</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/optimizers.html">fl_sim.optimizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.optimizers.get_optimizer.html">fl_sim.optimizers.get_optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.optimizers.get_optimizer.html#fl_sim.optimizers.get_optimizer"><code class="docutils literal notranslate"><span class="pre">get_optimizer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.optimizers.register_optimizer.html">fl_sim.optimizers.register_optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.optimizers.register_optimizer.html#fl_sim.optimizers.register_optimizer"><code class="docutils literal notranslate"><span class="pre">register_optimizer()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/regularizers.html">fl_sim.regularizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.get_regularizer.html">fl_sim.regularizers.get_regularizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.get_regularizer.html#fl_sim.regularizers.get_regularizer"><code class="docutils literal notranslate"><span class="pre">get_regularizer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.Regularizer.html">Regularizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.Regularizer.html#fl_sim.regularizers.Regularizer"><code class="docutils literal notranslate"><span class="pre">Regularizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.Regularizer.html#fl_sim.regularizers.Regularizer.eval"><code class="docutils literal notranslate"><span class="pre">Regularizer.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.Regularizer.html#fl_sim.regularizers.Regularizer.extra_repr_keys"><code class="docutils literal notranslate"><span class="pre">Regularizer.extra_repr_keys()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.Regularizer.html#fl_sim.regularizers.Regularizer.prox_eval"><code class="docutils literal notranslate"><span class="pre">Regularizer.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.L1Norm.html">L1Norm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.L1Norm.html#fl_sim.regularizers.L1Norm"><code class="docutils literal notranslate"><span class="pre">L1Norm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L1Norm.html#fl_sim.regularizers.L1Norm.eval"><code class="docutils literal notranslate"><span class="pre">L1Norm.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L1Norm.html#fl_sim.regularizers.L1Norm.prox_eval"><code class="docutils literal notranslate"><span class="pre">L1Norm.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2Norm.html">L2Norm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2Norm.html#fl_sim.regularizers.L2Norm"><code class="docutils literal notranslate"><span class="pre">L2Norm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2Norm.html#fl_sim.regularizers.L2Norm.eval"><code class="docutils literal notranslate"><span class="pre">L2Norm.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2Norm.html#fl_sim.regularizers.L2Norm.prox_eval"><code class="docutils literal notranslate"><span class="pre">L2Norm.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2NormSquared.html">L2NormSquared</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2NormSquared.html#fl_sim.regularizers.L2NormSquared"><code class="docutils literal notranslate"><span class="pre">L2NormSquared</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2NormSquared.html#fl_sim.regularizers.L2NormSquared.eval"><code class="docutils literal notranslate"><span class="pre">L2NormSquared.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.L2NormSquared.html#fl_sim.regularizers.L2NormSquared.prox_eval"><code class="docutils literal notranslate"><span class="pre">L2NormSquared.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.LInfNorm.html">LInfNorm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.LInfNorm.html#fl_sim.regularizers.LInfNorm"><code class="docutils literal notranslate"><span class="pre">LInfNorm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.LInfNorm.html#fl_sim.regularizers.LInfNorm.eval"><code class="docutils literal notranslate"><span class="pre">LInfNorm.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.LInfNorm.html#fl_sim.regularizers.LInfNorm.prox_eval"><code class="docutils literal notranslate"><span class="pre">LInfNorm.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/generated/fl_sim.regularizers.NullRegularizer.html">NullRegularizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/generated/fl_sim.regularizers.NullRegularizer.html#fl_sim.regularizers.NullRegularizer"><code class="docutils literal notranslate"><span class="pre">NullRegularizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.NullRegularizer.html#fl_sim.regularizers.NullRegularizer.eval"><code class="docutils literal notranslate"><span class="pre">NullRegularizer.eval()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/generated/fl_sim.regularizers.NullRegularizer.html#fl_sim.regularizers.NullRegularizer.prox_eval"><code class="docutils literal notranslate"><span class="pre">NullRegularizer.prox_eval()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Federated learning algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview-of-optimization-algorithms-in-federated-learning">Overview of Optimization Algorithms in Federated Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#federated-averaging-algorithm">Federated Averaging Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fedavg-from-the-perspective-of-optimization"><code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> from the Perspective of Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#a-direct-improvement-of-fedavg">A Direct Improvement of <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#proximal-algorithms-in-federated-learning">Proximal Algorithms in Federated Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#primal-dual-algorithms-in-federated-learning">Primal-Dual Algorithms in Federated Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#operator-splitting-algorithms-in-federated-learning">Operator Splitting Algorithms in Federated Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#skipping-algorithms-in-federated-learning">Skipping Algorithms in Federated Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command line interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="viz.html">Visualization subsystem</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">fl-sim</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Federated learning algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/wenh06/fl-sim/blob/master/docs/source/algorithms.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="federated-learning-algorithms">
<h1>Federated learning algorithms<a class="headerlink" href="#federated-learning-algorithms" title="Permalink to this heading">¶</a></h1>
<section id="overview-of-optimization-algorithms-in-federated-learning">
<span id="fl-alg-overview"></span><h2>Overview of Optimization Algorithms in Federated Learning<a class="headerlink" href="#overview-of-optimization-algorithms-in-federated-learning" title="Permalink to this heading">¶</a></h2>
<p>Federated Optimization algorithms have been the central problem in the field of federated learning since its inception.
The most important contribution of the initial work on federated learning <span id="id1">[<a class="reference internal" href="#id33" title="Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Artificial Intelligence and Statistics, 1273–1282. PMLR, 2017.">MMR+17</a>]</span> was the introduction of the Federated Averaging algorithm (<code class="docutils literal notranslate"><span class="pre">FedAvg</span></code>).</p>
<p>Mathematically, federated learning solves the following problem of minimization of empirical risk function</p>
<div class="math notranslate nohighlight" id="equation-fl-basic-dist">
<span class="eqno">(1)<a class="headerlink" href="#equation-fl-basic-dist" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\DeclareMathOperator*{\expectation}{\mathbb{E}}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\R}{\mathbb{R}}\\\begin{split}\begin{array}{cl}
\minimize\limits_{\theta \in \R^d} &amp; f(\theta) = \expectation\limits_{k \sim {\mathcal{P}}} [f_k(\theta)], \\
\text{where} &amp; f_k(\theta) = \expectation\limits_{(x, y) \sim \mathcal{D}_k} [\ell_k(\theta; x, y)],
\end{array}\end{split}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\ell_k\)</span> is the loss function of client <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(\mathcal{D}_k\)</span> is the data distribution of client <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(\mathcal{P}\)</span> is the distribution of clients, and <span class="math notranslate nohighlight">\(\mathbb{E}\)</span> is the expectation operator.
If we simply let <span class="math notranslate nohighlight">\(\mathcal{P} = \{1, 2, \ldots, K\}\)</span>, then the optimization problem can be simplified as</p>
<div class="math notranslate nohighlight" id="equation-fl-basic">
<span class="eqno">(2)<a class="headerlink" href="#equation-fl-basic" title="Permalink to this equation">¶</a></span>\[\begin{array}{cl}
\minimize\limits_{\theta \in \R^d} &amp; f(\theta) = \sum\limits_{k=1}^K w_k f_k(\theta).
\end{array}\]</div>
<p>For further simplicity, we often take <span class="math notranslate nohighlight">\(w_k = \frac{1}{K}\)</span>. The functions <span class="math notranslate nohighlight">\(f_k\)</span> and <span class="math notranslate nohighlight">\(f\)</span> are usually assumed to satisfy the following conditions:</p>
<blockquote>
<div><ul>
<li><p>(A1) <span class="math notranslate nohighlight">\(f_k\)</span> and <span class="math notranslate nohighlight">\(f\)</span> are <span class="math notranslate nohighlight">\(L\)</span>-smooth (<span class="math notranslate nohighlight">\(L &gt; 0\)</span>), i.e. they have <span class="math notranslate nohighlight">\(L\)</span>-Lipschitz continuous gradients:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-l-smooth">
<span class="eqno">(3)<a class="headerlink" href="#equation-l-smooth" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{c}
\lVert \nabla f (\theta) - f (\theta') \rVert \leqslant L \lVert \theta - \theta' \rVert, \\
\lVert \nabla f_k (\theta) - f_k (\theta') \rVert \leqslant L \lVert \theta - \theta' \rVert,
\end{array}
\quad \forall \theta, \theta' \in \R^d, k = 1, \ldots, K.\end{split}\]</div>
</div></blockquote>
</li>
<li><p>(A2) The range of <span class="math notranslate nohighlight">\(f\)</span></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\DeclareMathOperator*{\dom}{dom}\\\dom(f) := \{ \theta \in \R^d ~|~ f(\theta) &lt; + \infty \}\end{aligned}\end{align} \]</div>
<p>is nonempty and lower bounded, i.e. there exists a constant <span class="math notranslate nohighlight">\(c \in \R\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-lower-bounded">
<span class="eqno">(4)<a class="headerlink" href="#equation-lower-bounded" title="Permalink to this equation">¶</a></span>\[f(\theta) \geqslant c &gt; -\infty, ~ \forall \theta \in \R^d,\]</div>
<p>or equivalently,</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-lower-bounded-2">
<span class="eqno">(5)<a class="headerlink" href="#equation-lower-bounded-2" title="Permalink to this equation">¶</a></span>\[f^* := \inf\limits_{\theta \in \R^d} f(\theta) &gt; - \infty.\]</div>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>In many cases, in order to facilitate the analysis of convergence, we will also make some assumptions about the gradient of the objective function:</p>
<blockquote>
<div><ul>
<li><p>(A3) Bounded gradient: there exists a constant <span class="math notranslate nohighlight">\(G &gt; 0\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-bdd-grad">
<span class="eqno">(6)<a class="headerlink" href="#equation-bdd-grad" title="Permalink to this equation">¶</a></span>\[\lVert \nabla f_k (\theta) \rVert^2 \leqslant G^2, ~ \forall \theta \in \R^d, ~ k = 1, \ldots K.\]</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>And the following assumptions on data distributions:</p>
<blockquote>
<div><ul>
<li><p>(A4-1) Data distribution is I.I.D. (identically and independently distributed) across clients, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-iid-1">
<span class="eqno">(7)<a class="headerlink" href="#equation-iid-1" title="Permalink to this equation">¶</a></span>\[\nabla f(\theta) = \expectation [f_k(\theta)] = \expectation\limits_{(x, y) \sim \mathcal{D}_k}[\nabla \ell_k(\theta; x, y)], ~ \forall \theta \in \R^d, ~ k = 1, \ldots K,\]</div>
<p>or equivalently, for any <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>, there exists a constant <span class="math notranslate nohighlight">\(B \geqslant 0\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-iid-2">
<span class="eqno">(8)<a class="headerlink" href="#equation-iid-2" title="Permalink to this equation">¶</a></span>\[\sum\limits_{k=1}^K \lVert \nabla f_k(\theta) \rVert^2 = \lVert f(\theta) \rVert^2, ~ \forall \theta \in \left\{ \theta \in \R^d ~ \middle| ~ \lVert f(\theta) \rVert^2 &gt; \varepsilon \right\}.\]</div>
</div></blockquote>
</li>
<li><p>(A4-2) Data distribution is non-I.I.D across clients, in which case we need a quantity to measure the degree of this statistical heterogeneity. This quantity can be defined in a number of ways <span id="id2">[<a class="reference internal" href="#id43" title="Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In International Conference on Machine Learning, 5132–5143. PMLR, 2020.">KKM+20</a>, <a class="reference internal" href="#id42" title="Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated Optimization in Heterogeneous Networks. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning and Systems, volume 2, 429–450. 2020.">LSZ+20</a>, <a class="reference internal" href="#id38" title="Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of FedAvg on Non-IID Data. In International Conference on Learning Representations. OpenReview.net, 2020.">LHY+20</a>, <a class="reference internal" href="#id50" title="Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data. IEEE Transactions on Signal Processing, 69:6055–6070, 2021. doi:10.1109/tsp.2021.3115952.">ZHD+21</a>]</span>. For example, in <span id="id3">[<a class="reference internal" href="#id43" title="Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In International Conference on Machine Learning, 5132–5143. PMLR, 2020.">KKM+20</a>]</span> and <span id="id4">[<a class="reference internal" href="#id50" title="Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data. IEEE Transactions on Signal Processing, 69:6055–6070, 2021. doi:10.1109/tsp.2021.3115952.">ZHD+21</a>]</span>, the so-called bounded gradient dissimilarity (BGD), denoted as <span class="math notranslate nohighlight">\((G; B)\)</span>-BGD, is used as this quantity. More specifically, there exists constants <span class="math notranslate nohighlight">\(G &gt; 0\)</span> and <span class="math notranslate nohighlight">\(B \geqslant 0\)</span> such that</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-bdd-grad-dissim">
<span class="eqno">(9)<a class="headerlink" href="#equation-bdd-grad-dissim" title="Permalink to this equation">¶</a></span>\[\dfrac{1}{K} \sum\limits_{k=1}^K \lVert \nabla f_k(\theta) \rVert^2 \leqslant G^2 + B^2 \lVert \nabla f(\theta) \rVert^2, ~ \forall \theta \in \R^d.\]</div>
<p>It should be noted that letting <span class="math notranslate nohighlight">\(B = 0\)</span>, the bounded gradient dissimilarity condition (A4-2) degenrates to the bounded gradient condition (A3).</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>Sometimes, in the proof of algorithm convergence, one needs to make assumptions on the convexity of the objective function <span class="math notranslate nohighlight">\(f\)</span>, which can be defined as follows:</p>
<blockquote>
<div><ul>
<li><p>(A5-1) convexity:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-def-convex-function">
<span class="eqno">(10)<a class="headerlink" href="#equation-def-convex-function" title="Permalink to this equation">¶</a></span>\[f(a \theta + (1 - a) \theta') \leqslant a f(\theta) + (1 - a) f(\theta'), ~ \forall \theta, \theta' \in \mathcal{C}, ~ \alpha \in [0, 1].\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is a convex set on which <span class="math notranslate nohighlight">\(f\)</span> is defined.</p>
</div></blockquote>
</li>
<li><p>(A5-2) Strong convexity: there exists a constant <span class="math notranslate nohighlight">\(\mu &gt; 0\)</span> such that <span class="math notranslate nohighlight">\(f - \frac{\mu}{2} \lVert \theta \rVert^2\)</span> is convex. In this case, we say that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex.</p></li>
</ul>
</div></blockquote>
<p>Due to the natural layered and decoupled structure of the federal learning problem, it is more natural to consider the following constrained optimization problem:</p>
<div class="math notranslate nohighlight" id="equation-fl-basic-constraint">
<span class="eqno">(11)<a class="headerlink" href="#equation-fl-basic-constraint" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{cl}
\minimize &amp; \frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k), \\
\text{subject to} &amp; \theta_k = \theta, ~ k = 1, \ldots, K.
\end{array}\end{split}\]</div>
<p>It is easy to find the equivalence between the constrained optimization problem <a class="reference internal" href="#equation-fl-basic-constraint">(11)</a>
and the unconstrained optimization problem <a class="reference internal" href="#equation-fl-basic">(2)</a>. The constrained formulation
<a class="reference internal" href="#equation-fl-basic-constraint">(11)</a> is called the <strong>consensus problem</strong> in the literature of distributed optimization <span id="id5">[<a class="reference internal" href="#id21" title="Stephen Boyd, Neal Parikh, and Eric Chu. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Now Publishers Inc., 2011.">BPC11</a>]</span>. The superiority of the constrained formulation <a class="reference internal" href="#equation-fl-basic-constraint">(11)</a> is that
the objective function becomes block-separable, which is more suitable for the design of parallel and distributed algorithms.</p>
<section id="federated-averaging-algorithm">
<h3>Federated Averaging Algorithm<a class="headerlink" href="#federated-averaging-algorithm" title="Permalink to this heading">¶</a></h3>
<p>The core idea of the <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> algorithm is to make full use of the local computation resources of each client
so that each client can perform multiple local iterations before uploading the local model to the server.
It alleviates the problem of straggler clients and reduces the communication overhead,
hence accelerating the convergence of the algorithm. This may well be thought of as a simple form of
<strong>skipping</strong> algorithm, which were further developed in <span id="id6">[<a class="reference internal" href="#id78" title="Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally! In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, 15750–15769. PMLR, 7 2022.">MMSR22</a>, <a class="reference internal" href="#id79" title="Siqi Zhang and Nicolas Loizou. ProxSkip for Stochastic Variational Inequalities: A Federated Learning Algorithm for Provable Communication Acceleration. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop). 2022.">ZL22</a>, <a class="reference internal" href="#id50" title="Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data. IEEE Transactions on Signal Processing, 69:6055–6070, 2021. doi:10.1109/tsp.2021.3115952.">ZHD+21</a>]</span>.
Pseudocode for <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> is shown as follows:</p>
<img alt="Psuedocode for ``FedAvg``" class="no-scaled-link align-center" id="pseduocode-fedavg" src="_images/fedavg.svg" width="80%" /><p><code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> achieved some good numerical results (see Section 3 of <span id="id7">[<a class="reference internal" href="#id33" title="Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Artificial Intelligence and Statistics, 1273–1282. PMLR, 2017.">MMR+17</a>]</span>),
but its convergence, espcially under non-I.I.D. data distributions, is not properly analyzed
(see <span id="id8">[<a class="reference internal" href="#id36" title="Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First Analysis of Local GD on Heterogeneous Data. arXiv preprint arXiv:1909.04715v2, 9 2019. doi:10.48550/ARXIV.1909.04715.">KMR19</a>, <a class="reference internal" href="#id37" title="Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter Theory for Local SGD on Identical and Heterogeneous Data. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, 4519–4529. PMLR, 8 2020.">KMR20</a>]</span>). There are several works that deal with this issue
(such as <span id="id9">[<a class="reference internal" href="#id38" title="Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of FedAvg on Non-IID Data. In International Conference on Learning Representations. OpenReview.net, 2020.">LHY+20</a>, <a class="reference internal" href="#id100" title="Fan Zhou and Guojing Cong. On the Convergence Properties of a K-Step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI'18, 3219–3227. AAAI Press, 2018.">ZC18</a>]</span>) with extra assumptions such as
the convexity of the objective function <span class="math notranslate nohighlight">\(f\)</span>, etc.</p>
</section>
<section id="fedavg-from-the-perspective-of-optimization">
<h3><code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> from the Perspective of Optimization<a class="headerlink" href="#fedavg-from-the-perspective-of-optimization" title="Permalink to this heading">¶</a></h3>
<p>In this section, we will analyze the <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> algorithm from the perspective of optimization theory.
In fact, the optimization problem <a class="reference internal" href="#equation-fl-basic">(2)</a> that <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> solves can be equivalently reformulated
as the following constrained optimization problem:</p>
<div class="math notranslate nohighlight" id="equation-fedavg-constraint">
<span class="eqno">(12)<a class="headerlink" href="#equation-fedavg-constraint" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\newcommand{\col}{\operatorname{col}}\\\begin{split}\begin{array}{cl}
\minimize &amp; F(\Theta) := \frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k), \\
\text{subject to} &amp; \Theta \in \mathcal{E},
\end{array}\end{split}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta = \col(\theta_1, \cdots, \theta_K) := \begin{pmatrix} \theta_1 \\ \vdots \\ \theta_K \end{pmatrix}, \theta_1, \ldots, \theta_K \in \R^d\)</span>
and <span class="math notranslate nohighlight">\(\mathcal{E} = \left\{ \Theta ~ \middle| ~ \theta_1 = \cdots = \theta_K \right\}\)</span> is a convex set in <span class="math notranslate nohighlight">\(\R^{Kd}\)</span>.
Projected gradient descent (PGD) is an effective method for solving the constrained optimization problem <a class="reference internal" href="#equation-fedavg-constraint">(12)</a>, which has the following update rule:</p>
<div class="math notranslate nohighlight" id="equation-fedavg-pgd">
<span class="eqno">(13)<a class="headerlink" href="#equation-fedavg-pgd" title="Permalink to this equation">¶</a></span>\[\Theta^{(t+1)} = \Pi_{\mathcal{E}} \left( \Theta^{(t)} - \eta \nabla F(\Theta^{(t)}) \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi_{\mathcal{E}}\)</span> is the projection operator onto the set <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>. It is easy to show that
the projection operator onto the set <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> is indeed the average operator, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-fedavg-projection">
<span class="eqno">(14)<a class="headerlink" href="#equation-fedavg-projection" title="Permalink to this equation">¶</a></span>\[\Pi_{\mathcal{E}}: \R^{Kd} \to \mathcal{E}: ( \theta_1, \ldots, \theta_K) \mapsto \left(\frac{1}{K}\sum\limits_{k=1}^K \theta_K, \ldots, \frac{1}{K}\sum\limits_{k=1}^K \theta_K \right).\]</div>
<p>We have shown that mathematically the <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> algorithm is indeed a kind of stochastic projected gradient descent (SPGD)
algorithm, where the clients perform local stochastic gradient descent (SGD) updates and the server performs
the projection step <a class="reference internal" href="#equation-fedavg-projection">(14)</a>.</p>
</section>
<section id="a-direct-improvement-of-fedavg">
<h3>A Direct Improvement of <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code><a class="headerlink" href="#a-direct-improvement-of-fedavg" title="Permalink to this heading">¶</a></h3>
<p>Since <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> is based on stochastic gradient descent (SGD), it is natural to consider applying
acceleration techniques <span id="id10">[<a class="reference internal" href="#id31" title="John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12(61):2121–2159, 7 2011.">DHS11</a>, <a class="reference internal" href="#id19" title="Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. 2015.">KB15</a>, <a class="reference internal" href="#id57" title="Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In 6th International Conference on Learning Representations (ICLR). OpenReview.net, 5 2018. URL: https://openreview.net/forum?id=ryQu7f-RZ.">RKK18</a>, <a class="reference internal" href="#id32" title="Manzil Zaheer, Sashank J. Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive Methods for Nonconvex Optimization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, 9815–9825. Red Hook, NY, USA, 2018. Curran Associates Inc.">ZRS+18</a>]</span> to improve the algorithm performance.
Computation on clients and on the server are relatively decoupled, so it does not require large modifications
to the whole algorithm framework. Indeed, the authors of the <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> paper put this idea into practice and proposed
a federated learning framework called <code class="docutils literal notranslate"><span class="pre">FedOpt</span></code> <span id="id11">[<a class="reference internal" href="#id39" title="Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive Federated Optimization. In International Conference on Learning Representations. 2021.">RCZ+21</a>]</span> which has stronger adaptability.
The pseudocode for <code class="docutils literal notranslate"><span class="pre">FedOpt</span></code> is shown as follows:</p>
<img alt="Psuedocode for ``FedOpt``" class="no-scaled-link align-center" id="pseduocode-fedopt" src="_images/fedopt.svg" width="80%" /><p>In the above pseudocode, <span class="math notranslate nohighlight">\(\operatorname{aggregate} \left( \left\{ \Delta_{k}^{(t)} \right\}_{k \in \mathcal{S}^{(t)}} \right)\)</span>
refers to some method that aggregates the local inertia updates <span class="math notranslate nohighlight">\(\Delta_{k}^{(t)}\)</span> from the selected clients
<span class="math notranslate nohighlight">\(\mathcal{S}^{(t)}\)</span> into a global inertia update <span class="math notranslate nohighlight">\(\Delta^{(t)}\)</span>. This method, for example, can be simply averaging</p>
<div class="math notranslate nohighlight" id="equation-fedopt-agg-inertia-average">
<span class="eqno">(15)<a class="headerlink" href="#equation-fedopt-agg-inertia-average" title="Permalink to this equation">¶</a></span>\[\Delta^{(t)} \gets \frac{1}{\lvert \mathcal{S}^{(t)} \rvert} \sum\limits_{k \in \mathcal{S}^{(t)}} \Delta_{k}^{(t)},\]</div>
<p>or linear combination with inertia of the previous iteration</p>
<div class="math notranslate nohighlight" id="equation-fedopt-agg-inertia-lin-comb">
<span class="eqno">(16)<a class="headerlink" href="#equation-fedopt-agg-inertia-lin-comb" title="Permalink to this equation">¶</a></span>\[\Delta^{(t)} \gets \beta_1 \Delta^{(t-1)} + \left( (1 - \beta_1) / \lvert \mathcal{S}^{(t)} \rvert \right) \sum_{k \in \mathcal{S}^{(t)}} \Delta_{k}^{(t)}.\]</div>
<p>As one has already noticed, compared to <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code>, <code class="docutils literal notranslate"><span class="pre">FedOpt</span></code> introduces some momentum terms on the server node (in <strong>ServerOpt</strong>) to
accelerate the convergence. In <span id="id12">[<a class="reference internal" href="#id39" title="Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive Federated Optimization. In International Conference on Learning Representations. 2021.">RCZ+21</a>]</span>, the authors listed several options for <strong>ServerOpt</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">FedAdagrad</span></code>:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-fedopt-serveropt-fedadagrad">
<span class="eqno">(17)<a class="headerlink" href="#equation-fedopt-serveropt-fedadagrad" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
v^{(t)} &amp; \gets v^{(t-1)} + ( \Delta^{(t)} )^2 \\
\theta^{(t+1)} &amp; \gets \theta^{(t)} + \eta_g \Delta^{(t)} / (\sqrt{v^{(t)}}+\tau)
\end{aligned}\end{split}\]</div>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">FedYogi</span></code>:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-fedopt-serveropt-fedyogi">
<span class="eqno">(18)<a class="headerlink" href="#equation-fedopt-serveropt-fedyogi" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
v^{(t)} &amp; \gets v^{(t-1)} - (1 - \beta_2) ( \Delta^{(t)} )^2 \operatorname{sign}(v^{(t-1)} - ( \Delta^{(t)} )^2) \\
\theta^{(t+1)} &amp; \gets \theta^{(t)} + \eta_g \Delta^{(t)} / (\sqrt{v^{(t)}}+\tau)
\end{aligned}\end{split}\]</div>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">FedAdam</span></code>:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-fedopt-serveropt-fedadam">
<span class="eqno">(19)<a class="headerlink" href="#equation-fedopt-serveropt-fedadam" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
v^{(t)} &amp; \gets \beta_2 v^{(t-1)} + (1 - \beta_2) ( \Delta^{(t)} )^2 \\
\theta^{(t+1)} &amp; \gets \theta^{(t)} + \eta_g \Delta^{(t)} / (\sqrt{v^{(t)}}+\tau)
\end{aligned}\end{split}\]</div>
</div></blockquote>
</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">FedOpt</span></code> applys acceleration techniques which are frequently used in general machine learning tasks to the field of
federated learning. It is a direct improvement of <code class="docutils literal notranslate"><span class="pre">FedAvg</span></code> which is simple but important. Moreover, it demonstrates
the decoupling of the computation on clients and on the server, which is a key feature of federated learning.</p>
<p>to write more ….</p>
<img alt="Psuedocode for ``Scaffold``" class="no-scaled-link align-center" id="pseduocode-scaffold" src="_images/scaffold.svg" width="80%" /></section>
</section>
<section id="proximal-algorithms-in-federated-learning">
<h2>Proximal Algorithms in Federated Learning<a class="headerlink" href="#proximal-algorithms-in-federated-learning" title="Permalink to this heading">¶</a></h2>
<p>In non-I.I.D. scenarios, based on the idea of reducing the impact of local updates of clients on the global model,
<span id="id13">[<a class="reference internal" href="#id42" title="Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated Optimization in Heterogeneous Networks. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, Proceedings of Machine Learning and Systems, volume 2, 429–450. 2020.">LSZ+20</a>]</span> first introduced a proximal term to the local objective functions, aiming at making the
algorithm more stable and converging faster.</p>
<div class="figure" id="id148" style="text-align: center">
<span id="fig-apfl"></span><p><img width="80%" src="_images/tikz-a4a11a93d19f47c1e3a684dd5159c5d79f350276.svg" alt="Figure made with TikZ" /></p>
<p><span class="caption-text">Schematic diagram for <span class="math notranslate nohighlight">\(f_k(\alpha_k \omega_k + (1 - \alpha_k) \theta^*)\)</span> in the APFL algorithm.</span></p>
</div><div class="figure" id="id149" style="text-align: center">
<span id="fig-feddyn"></span><p><img width="80%" src="_images/tikz-ea5939230cbfad9f08b2efbb2087d0f52426b2e4.svg" alt="Figure made with TikZ" /></p>
<p><span class="caption-text">Client model parameter update schematic diagram of the FedDyn algorithm.</span></p>
</div><p>to write….</p>
</section>
<section id="primal-dual-algorithms-in-federated-learning">
<span id="fl-alg-primal-dual"></span><h2>Primal-Dual Algorithms in Federated Learning<a class="headerlink" href="#primal-dual-algorithms-in-federated-learning" title="Permalink to this heading">¶</a></h2>
<p>to write….</p>
</section>
<section id="operator-splitting-algorithms-in-federated-learning">
<span id="fl-alg-operator-splitting"></span><h2>Operator Splitting Algorithms in Federated Learning<a class="headerlink" href="#operator-splitting-algorithms-in-federated-learning" title="Permalink to this heading">¶</a></h2>
<p>to write….</p>
</section>
<section id="skipping-algorithms-in-federated-learning">
<span id="fl-alg-skipping"></span><h2>Skipping Algorithms in Federated Learning<a class="headerlink" href="#skipping-algorithms-in-federated-learning" title="Permalink to this heading">¶</a></h2>
<p>to write….</p>
<div class="docutils container" id="id14">
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">BPC11</a><span class="fn-bracket">]</span></span>
<p>Stephen Boyd, Neal Parikh, and Eric Chu. <em>Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</em>. Now Publishers Inc., 2011.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">DHS11</a><span class="fn-bracket">]</span></span>
<p>John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. <em>Journal of Machine Learning Research</em>, 12(61):2121–2159, 7 2011.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KKM+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. In <em>International Conference on Machine Learning</em>, 5132–5143. PMLR, 2020.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">KMR19</a><span class="fn-bracket">]</span></span>
<p>Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. First Analysis of Local GD on Heterogeneous Data. <em>arXiv preprint arXiv:1909.04715v2</em>, 9 2019. <a class="reference external" href="https://doi.org/10.48550/ARXIV.1909.04715">doi:10.48550/ARXIV.1909.04715</a>.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">KMR20</a><span class="fn-bracket">]</span></span>
<p>Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter Theory for Local SGD on Identical and Heterogeneous Data. In Silvia Chiappa and Roberto Calandra, editors, <em>Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</em>, volume 108 of Proceedings of Machine Learning Research, 4519–4529. PMLR, 8 2020.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">KB15</a><span class="fn-bracket">]</span></span>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In <em>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>. 2015.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LSZ+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p>Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated Optimization in Heterogeneous Networks. In I. Dhillon, D. Papailiopoulos, and V. Sze, editors, <em>Proceedings of Machine Learning and Systems</em>, volume 2, 429–450. 2020.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LHY+20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of FedAvg on Non-IID Data. In <em>International Conference on Learning Representations</em>. OpenReview.net, 2020.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MMR+17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p>Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In <em>Artificial Intelligence and Statistics</em>, 1273–1282. PMLR, 2017.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">MMSR22</a><span class="fn-bracket">]</span></span>
<p>Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally! In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, <em>Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of Proceedings of Machine Learning Research, 15750–15769. PMLR, 7 2022.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RCZ+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive Federated Optimization. In <em>International Conference on Learning Representations</em>. 2021.</p>
</div>
<div class="citation" id="id57" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">RKK18</a><span class="fn-bracket">]</span></span>
<p>Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In <em>6th International Conference on Learning Representations (ICLR)</em>. OpenReview.net, 5 2018. URL: <a class="reference external" href="https://openreview.net/forum?id=ryQu7f-RZ">https://openreview.net/forum?id=ryQu7f-RZ</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">ZRS+18</a><span class="fn-bracket">]</span></span>
<p>Manzil Zaheer, Sashank J. Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive Methods for Nonconvex Optimization. In <em>Proceedings of the 32nd International Conference on Neural Information Processing Systems</em>, NIPS'18, 9815–9825. Red Hook, NY, USA, 2018. Curran Associates Inc.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">ZL22</a><span class="fn-bracket">]</span></span>
<p>Siqi Zhang and Nicolas Loizou. ProxSkip for Stochastic Variational Inequalities: A Federated Learning Algorithm for Provable Communication Acceleration. In <em>OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)</em>. 2022.</p>
</div>
<div class="citation" id="id50" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ZHD+21<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id6">3</a>)</span>
<p>Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A Federated Learning Framework With Adaptivity to Non-IID Data. <em>IEEE Transactions on Signal Processing</em>, 69:6055–6070, 2021. <a class="reference external" href="https://doi.org/10.1109/tsp.2021.3115952">doi:10.1109/tsp.2021.3115952</a>.</p>
</div>
<div class="citation" id="id100" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">ZC18</a><span class="fn-bracket">]</span></span>
<p>Fan Zhou and Guojing Cong. On the Convergence Properties of a K-Step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization. In <em>Proceedings of the 27th International Joint Conference on Artificial Intelligence</em>, IJCAI'18, 3219–3227. AAAI Press, 2018.</p>
</div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cli.html" class="btn btn-neutral float-right" title="Command line interface" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="api/generated/fl_sim.regularizers.NullRegularizer.html" class="btn btn-neutral" title="NullRegularizer" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, WEN Hao.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> and ❤️  using a custom <a href="https://github.com/LinxiFan/Stanford-theme">theme</a> based on <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/documentation_options.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="None"></script>
      <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>